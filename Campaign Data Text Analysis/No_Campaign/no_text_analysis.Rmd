---
title: "no_text_analysis"
output: html_document
date: "2023-07-17"
---
Goal: Perform text analysis on Yes vs. No on Prop 22 campaigns.
```{r setup, include=FALSE}
library(easypackages)
libraries("tidyverse", "rvest", "httr", "tm", "topicmodels", "RColorBrewer", "wordcloud", "tidytext", "syuzhet", "gplots", "ggplot2")
```
Scrape data from Opponents
```{r}
### Main page: https://web.archive.org/web/20201101075944/https://nooncaprop22.com/
main_oppose_url <- read_html("https://web.archive.org/web/20201101075944/https://nooncaprop22.com/")

main_oppose_text <- main_oppose_url %>%
  html_nodes(xpath = "/html/body") %>%
  html_text()

pattern1_oppose <- "Uber, Lyft, and DoorDash spent millions to write a deceptive measure and qualify it for the November ballot. They hired expensive lawyers and paid political operatives to collect enough signatures to ask California voters just one question: Will you let app companies buy themselves a special exemption in the law to exploit workers for profit?"
pattern2_oppose <- "Current law requires app companies to provide their drivers with basic benefits and protections - like paid sick leave and unemployment insurance. But the billion-dollar companies don’t want to pay, so they’re buying themselves a special exemption in the law to avoid ever having to pay their fair share to keep drivers safe on the job."
pattern3_oppose <- "App companies built a billion-dollar empire on the backs of drivers, while refusing to provide them with the basic protections and benefits they are owed. These drivers - 78% from communities of color, and 70% working more than 30 hours a week - are essential, helping California through the global pandemic. They deserve better."
pattern4_oppose <- "The California Attorney General and City Attorneys from across the state are cracking down on Uber and Lyft for the years of violating workers’ rights laws, purposefully misclassifying drivers to avoid paying minimum wage, healthcare, paid sick leave, unemployment insurance, and workers’ compensation coverage."
pattern5_oppose <- "Join tens of thousands of drivers:"
pattern6_oppose <- "Slam the brakes on this cynical measure by voting NO on Prop 22!"

# Extract text chunks that match the pattern
patterns_oppose <- c(pattern1_oppose, pattern2_oppose, pattern3_oppose, pattern4_oppose, pattern5_oppose)

# Initialize an empty vector for the matched chunks
matched_chunks_oppose <- character(0)

# Iterate over each pattern
for (pattern in patterns_oppose) {
  # Extract text chunks that match the current pattern
  matched <- str_extract_all(main_oppose_text, pattern)
  # Append the matched chunks to the vector
  matched_chunks_oppose <- c(matched_chunks_oppose, unlist(matched))
}

# Create a dataframe with the matched chunks
matched_df_oppose <- data.frame(No_Campaign = matched_chunks_oppose)

## Decoding Prop 22: https://web.archive.org/web/20201101000523/https://nooncaprop22.com/decoding_22
decoding_url <- read_html("https://web.archive.org/web/20201101000523/https://nooncaprop22.com/decoding_22")

decoding_text <- decoding_url %>%
  html_nodes(xpath = "/html/body") %>%
  html_text()

pattern1_decoding <- "DON’T BE FOOLED - Uber, Lyft, and DoorDash are spending millions to hide the truth from voters about who benefits from Proposition 22. They make nice-sounding claims about how their million-dollar ballot measure could help drivers - but we checked out the fine print, and it turns out the companies will be the only ones who benefit; not drivers."
pattern2_decoding <- "Protects the choice of app-based drivers to work as independent contractors."
pattern3_decoding <- "Creates a loophole in existing law just for app-based companies to continue exploiting their workers for profit."
pattern4_decoding <- "Improves app-based work by requiring companies to provide new benefits."
pattern4_decoding <- "Lets app companies boost their profits by refusing to provide their drivers with the benefits required under current law like paid sick leave, unemployment insurance, or healthcare."
pattern5_decoding <- "Guarenteed minimum earnings"
pattern6_decoding <- "Prop 22 only requires app companies to pay drivers for “engaged time” - when they are logged in to an app, and actively working. That means drivers would only be guaranteed $5.64 an hour under Prop 22 - far less than minimum wage."
pattern7_decoding <- "Funding for health benefits"
pattern8_decoding <- "Caps coverage at only a fraction of the lowest-cost Covered California plan. Bases coverage on “engaged time,” forcing drivers to work far more than 39 hours a week just to qualify for the minimum healthcare benefit."
pattern9_decoding <- "Medical and disability coverage for on-the-job injuries"
pattern10_decoding <- "Prop 22 allows app companies to shove the cost of medical care for on-the-job-injuries onto workers, instead of providing workers’ compensation. Gives the app companies more power to deny coverage for their drivers, and caps disability benefits."
pattern11_decoding <- "Protections against harassment and discrimination"
pattern12_decoding <- "Prop 22 waters down existing protections for workers against harassment and discrimination by allowing for discrimination against immigration status, and failing to include any enforcement tools."
pattern13_decoding <- "Creates expanded public safety protections including: requiring background checks and safety courses"
pattern14_decoding <- "Weakens current protections for riders and drivers. Eliminates required sexual harassment training as well as the obligations on Uber and Lyft to investigate both customers’ and drivers’ harassment claims."

# Extract text chunks that match the pattern
patterns_decoding <- c(pattern1_decoding, pattern2_decoding, pattern3_decoding, pattern4_decoding, pattern5_decoding, pattern6_decoding, pattern7_decoding, pattern8_decoding, pattern9_decoding, pattern10_decoding, pattern11_decoding)

# Initialize an empty vector for the matched chunks
matched_chunks_decoding <- character(0)

# Iterate over each pattern
for (pattern in patterns_decoding) {
  # Extract text chunks that match the current pattern
  matched <- str_extract_all(decoding_text, pattern)
  # Append the matched chunks to the vector
  matched_chunks_decoding <- c(matched_chunks_decoding, unlist(matched))
}

# Create a dataframe with the matched chunks
matched_df_decoding <- data.frame(No_Campaign = matched_chunks_decoding)

## Who's Behind It: https://web.archive.org/web/20201101103731/https://nooncaprop22.com/whos_behind_it
authors_url <- read_html("https://web.archive.org/web/20201101103731/https://nooncaprop22.com/whos_behind_it")

authors_text <- authors_url %>%
  html_nodes(xpath = "/html/body/div[4]") %>%
  html_text()

pattern1_authors <- "New York - In 2015, Uber threatened to leave New York City over a dispute with the City Council."
pattern2_authors <- "Uber never left."
pattern3_authors <- "Austin, TX - In 2016, Uber and Lyft threatened to leave Austin if local voters failed to approve a measure they put on the ballot to loosen background check restrictions."
pattern4_authors <- "The companies returned just six months later."
pattern5_authors <- "Chicago, IL - In 2016, Uber threatened to abandon Chicago to avoid having to comply with the city’s licensing regulations, and Lyft joined in."
pattern6_authors <- "Neither company ever left."
pattern7_authors <- "Phoenix SkyHarbor Airport - In February this year, Uber and Lyft threatened to stop picking up passengers at Phoenix’s SkyHarbor airport if an increased airport pickup fee was allowed to stand. The fee went into effect on May 1 of this year."
pattern8_authors <- "Neither company has left the market."
pattern9_authors <- "California - Uber and Lyft have threatened to halt service in California after a judge ruled that the ride-hailing companies must immediately follow state law and classify their drivers as employees."
pattern10_authors <- "Uber and Lyft are now banking on being able to trick California voters with Prop 22 on November 3rd."

# Extract text chunks that match the pattern
patterns_authors <- c(pattern1_authors, pattern2_authors, pattern3_authors, pattern4_authors, pattern5_authors, pattern6_authors, pattern7_authors, pattern8_authors, pattern9_authors, pattern10_authors)

# Initialize an empty vector for the matched chunks
matched_chunks_authors <- character(0)

# Iterate over each pattern
for (pattern in patterns_authors) {
  # Extract text chunks that match the current pattern
  matched <- str_extract_all(authors_text, pattern)
  # Append the matched chunks to the vector
  matched_chunks_authors <- c(matched_chunks_authors, unlist(matched))
}

# Create a dataframe with the matched chunks
matched_df_authors <- data.frame(No_Campaign = matched_chunks_authors)

## Impact on Drivers: https://web.archive.org/web/20201101103731/https://nooncaprop22.com/impact_on_drivers
impact_url <- read_html("https://web.archive.org/web/20201101103731/https://nooncaprop22.com/impact_on_drivers")

impact_text <- impact_url %>%
  html_nodes(xpath = "/html/body") %>%
  html_text()

# Remove \t and \n from the text
impact_cleaned_text <- gsub("[\t\n]", "", impact_text)

# Find the index of the specific phrase
split_index <- strsplit(impact_cleaned_text, "Uber, Lyft, and DoorDash have stockpiled")[[1]]

# Get the portion before the specific phrase
impact_cleaned_text <- split_index[1]

# Example string with parts that need spaces
text <- "About Prop 22Decoding Prop 22Who's behind 22?Impact on DriversNewsOur CoalitionTake ActionShare on SocialEn EspañolDonateAbout Prop 22Decoding Prop 22Who's behind 22?Impact on DriversNewsOur CoalitionTake ActionShare on SocialEn EspañolDonateImpact on DriversUber, Lyft, and DoorDash wrote Prop 22 to have clear winners and losers; and drivers won’t be getting the upper hand. See how Proposition 22 will weaken the benefits and protections drivers are entitled to under the law.Current LawUnder Prop 22WagesClear minimum wage; guaranteed overtime (150% of wages for work over 8 hours in one day, 40 hours in one week)No overtime; sub-minimum wage likelyExpense ReimbursementAll expenses reimbursed (mileage, cell phones, car cleaning, etc.) – standard IRS rate is over 57 cents per mile.Thirty cents per mile, but only mileage expenses for “engaged” miles, (e.g. no reimbursement for time without package/passenger)Workers’ CompensationNo-fault coverage for work-related injuries.Limited health coverage; not “no-fault;” easier for insurers to deny coveragePaid Family Leave8 weeks of paid leaveNonePaid Sick DaysThree days of paid leave for illness or care of family – up to 10 in some cities; additional COVID-19 leave in some citiesNoneUnemployment CompensationUp to 26 weeks of cash benefits after no-fault job lossNoneDisability InsuranceLifetime access to wage replacement if injuredLimited - caps total coverage for 104 weeksHealth InsuranceAccess to federal benefits under the Affordable Care ActLimited - calculated based on “engaged” time, reducing the benefit amountDiscriminationProtection against discrimination based on a broad set of characteristicsNo explicit protection against discrimination based on immigration statusRight to Organize and Collectively BargainCould be created under state lawNone - and may only be afforded if state passes legislation by 7/8ths majority, which is nearly impossibleProtection from RetaliationProtection from termination or discipline for reporting harassment, discrimination, or wage theftNoneHealth and SafetyRequires companies to establish injury prevention plans; give workers access to sanitation facilitiesNo similar requirement"

# Insert spaces between specific parts using regular expressions
clean_text <- gsub("(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])", " ", text, perl = TRUE)

## Bind all dataframes
no_df <- rbind(matched_df_oppose, matched_df_decoding, matched_df_authors, clean_text)
saveRDS(no_df, file = "no_df.csv")
```
# Topic Modeling for No Campaign
# Resource: https://ladal.edu.au/topicmodels.html
```{r}
no_df <- readRDS("no_df.csv")
## Data pre-processing
# Create a corpus from the dataframe column
corpus <- Corpus(VectorSource(no_df$No_Campaign))

# Preprocessing: remove punctuation and convert to lowercase
corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE) # Remove punctuation
corpus <- tm_map(corpus, content_transformer(tolower)) # Change to lowercase
corpus <- tm_map(corpus, content_transformer(function(x) gsub("\\s+", " ", x))) # Remove extra spaces

# Remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("en"))

# Convert back to a dataframe
cleaned_no_text <- sapply(corpus, as.character)
no_df$No_Campaign_Cleaned <- cleaned_no_text

# Create the Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)

# Convert the DTM to a matrix
dtm_matrix <- as.matrix(dtm)

## LDA
# Create the LDA model
lda_model <- topicmodels::LDA(dtm, k = 5)

# Print the model summary
print(lda_model)

# Get the topics and associated words
topics <- terms(lda_model, 10)
topicNames <- apply(topics, 2, paste, collapse=" ")
print(topics)

# Examine topic-word probabilities
lda_model@beta

# Get the posterior probabilities of documents belonging to each topic
document_topics <- posterior(lda_model, newdata = dtm)

# Find the most relevant topic for each document
most_relevant_topic <- sapply(document_topics, function(x) which.max(x))
# 73 unique terms in the corpus; 25 topics identified

# visualize topics as word cloud
topicToViz <- 5 # change for your own topic of interest
topicToViz <- grep('provide', topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(document_topics$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(document_topics$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2") # Generates a vector of 5 colors from the rainbow palette

png("no_wordcloud.png")
no_wordcloud <- wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
dev.off()

## Generate a heat map
# Calculate the co-occurrence matrix
co_occurrence_matrix <- crossprod(dtm_matrix)
co_occurrence_matrix <- co_occurrence_matrix / rowSums(co_occurrence_matrix)
co_occurrence_matrix <- co_occurrence_matrix / colSums(co_occurrence_matrix)

# Create the heatmap
heatmap(co_occurrence_matrix, 
        col = colorRampPalette(c("blue", "white", "red"))(100),  # Color scheme
        main = "Co-occurrence Heatmap",  # Title of the heatmap
        xlab = "Tokens",  # Label for x-axis
        ylab = "Tokens",   # Label for y-axis
        scale = "none"
)

## Histogram
hist(top40terms, main = "Word Frequency Distribution", xlab = "Frequency", ylab = "Count")

## Bar plot
top10terms <- sort(document_topics$terms[topicToViz,], decreasing=TRUE)[1:10]

# Create a data frame with the top 40 terms and their frequencies
top10terms_df <- data.frame(Terms = names(top10terms), Frequency = top10terms, stringsAsFactors = FALSE)

# Round the frequencies to two decimal places
top10terms_df$Frequency <- round(top10terms_df$Frequency, 2)

# Create the bar plot using ggplot2
barchart <- ggplot(top10terms_df, aes(x = reorder(Terms, -Frequency), y = Frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Top 10 Most Frequent Words", x = "Words", y = "Frequency") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_text(aes(label = Frequency), vjust = -0.5, color = "black", size = 3)  # 
ggsave("barchart.png", plot = barchart)
```
Sentiment analysis
```{r}
## NRC
tidy_data <- no_df %>%
  unnest_tokens(word, No_Campaign_Cleaned)

sentiment_scores <- get_nrc_sentiment(tidy_data$word)

# Get total sentiment by sentiment category
total_sentiment <- colSums(sentiment_scores)

# Create dataframe
sentiment_nrc_df <- data.frame(Sentiment_Category = colnames(sentiment_scores), Total_Sentiment = total_sentiment)

# Sort dataframe by total sentiment in descending order
sentiment_nrc_df <- sentiment_nrc_df[order(sentiment_nrc_df$Total_Sentiment, decreasing = TRUE), ]
write.csv(sentiment_nrc_df, "sentiment_nrc_scores.csv")

## Vader
sentiment_vader <- vader_df(tidy_data$word)

# Calculate total frequency for each sentiment category
sentiment_totals <- sentiment_vader %>%
  summarise(Total_Compound = sum(compound),
            Total_Positive = sum(pos),
            Total_Neutral = sum(neu),
            Total_Negative = sum(neg))

# Create a dataframe with sentiment categories and total frequencies
sentiment_vader_df <- data.frame(Sentiment_Category = c("Compound", "Positive", "Neutral", "Negative"),
                           Total_Frequency = c(sentiment_totals$Total_Compound,
                                               sentiment_totals$Total_Positive,
                                               sentiment_totals$Total_Neutral,
                                               sentiment_totals$Total_Negative))
write.csv(sentiment_vader_df, "sentiment_vader_scores.csv")
```

```{r}
## PAM: No longer maintained :(
# Create the PAM model
pam_model <- pam(dtm, k = 5, control = list(seed = 895))

# Print the model summary
print(pam_model)

# Get the topics and associated words
topics <- terms(pam_model, 10)
print(topics)
```

```{r}
### Clusters

text_vector <- no_df$No_Campaign
corpus_cluster2 <- corpus(text_vector)

cluster_dfmno2 <- tokens(corpus_cluster2, remove_punct = TRUE, remove_numbers = TRUE, remove_separators = TRUE) %>%
  tokens_remove(stopwords("english")) %>%
  dfm()


clusters2 <- kmeans(cluster_dfmno2, 5)

clusters2

get_top_words <- function(centers, cluster, n = 3){
  (centers[cluster,] - colMeans(centers[-cluster,])) %>%
    sort(decreasing = TRUE) %>%
    head(n)
}

topwords <- get_top_words(clusters$centers, 1)

topwords2 <- get_top_words(clusters$centers, 2)

topwords3 <- get_top_words(clusters$centers, 3)
```
