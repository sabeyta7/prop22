# Insert spaces between specific parts using regular expressions
clean_text <- gsub("(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])", " ", text, perl = TRUE)
## Bind all dataframes
no_df <- rbind(matched_df_oppose, matched_df_decoding, matched_df_authors, clean_text)
saveRDS(no_df, file = "no_df.csv")
no_df <- readRDS("no_df.csv")
## Data pre-processing
# Create a corpus from the dataframe column
corpus <- Corpus(VectorSource(no_df$No_Campaign))
# Preprocessing: remove punctuation and convert to lowercase
corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE) # Remove punctuation
corpus <- tm_map(corpus, content_transformer(tolower)) # Change to lowercase
corpus <- tm_map(corpus, content_transformer(function(x) gsub("\\s+", " ", x))) # Remove extra spaces
# Remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# Convert back to a dataframe
cleaned_no_text <- sapply(corpus, as.character)
no_df$No_Campaign_Cleaned <- cleaned_no_text
# Create the Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
# Convert the DTM to a matrix
dtm_matrix <- as.matrix(dtm)
## LDA
# Create the LDA model
lda_model <- topicmodels::LDA(dtm, k = 5)
# Print the model summary
print(lda_model)
# Get the topics and associated words
topics <- terms(lda_model, 10)
topicNames <- apply(topics, 2, paste, collapse=" ")
print(topics)
# Examine topic-word probabilities
lda_model@beta
# Get the posterior probabilities of documents belonging to each topic
document_topics <- posterior(lda_model, newdata = dtm)
# Find the most relevant topic for each document
most_relevant_topic <- sapply(document_topics, function(x) which.max(x))
# 73 unique terms in the corpus; 25 topics identified
# visualize topics as word cloud
topicToViz <- 5 # change for your own topic of interest
topicToViz <- grep('provide', topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(document_topics$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(document_topics$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2") # Generates a vector of 5 colors from the rainbow palette
png("no_wordcloud.png")
no_wordcloud <- wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
words
words <- names(top40terms)
words
# visualize topics as word cloud
topicToViz <- 5 # change for your own topic of interest
topicToViz <- grep('provide', topicNames)[1] # Or select a topic by a term contained in its name
topicToViz
View(no_df)
corpus <- Corpus(VectorSource(no_df$No_Campaign))
# Preprocessing: remove punctuation and convert to lowercase
corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE) # Remove punctuation
corpus <- tm_map(corpus, content_transformer(tolower)) # Change to lowercase
corpus <- tm_map(corpus, content_transformer(function(x) gsub("\\s+", " ", x))) # Remove extra spaces
# Remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("en"))
cleaned_no_text <- sapply(corpus, as.character)
no_df$No_Campaign_Cleaned <- cleaned_no_text
# Create the Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
# Convert the DTM to a matrix
dtm_matrix <- as.matrix(dtm)
## LDA
# Create the LDA model
lda_model <- topicmodels::LDA(dtm, k = 5)
# Print the model summary
print(lda_model)
# Get the topics and associated words
topics <- terms(lda_model, 10)
topics
topicNames <- apply(topics, 2, paste, collapse=" ")
# Examine topic-word probabilities
lda_model@beta
# Get the posterior probabilities of documents belonging to each topic
document_topics <- posterior(lda_model, newdata = dtm)
# Find the most relevant topic for each document
most_relevant_topic <- sapply(document_topics, function(x) which.max(x))
most_relevant_topic
# Find the most relevant topic for each document
most_relevant_topic <- sapply(document_topics, function(x) which.max(x))
most_relevant_topic
# visualize topics as word cloud
topicToViz <- 5 # change for your own topic of interest
topicToViz <- grep('provide', topicNames)[1] # Or select a topic by a term contained in its name
topicToViz
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(document_topics$terms[topicToViz,], decreasing=TRUE)[1:40]
top40terms
words <- names(top40terms)
words
# extract the probabilites of each of the 40 terms
probabilities <- sort(document_topics$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2") # Generates a vector of 5 colors from the rainbow palette
png("no_wordcloud.png")
png("no_wordcloud.png")
no_wordcloud <- wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
dev.off()
## Generate a heat map
# Calculate the co-occurrence matrix
co_occurrence_matrix <- crossprod(dtm_matrix)
co_occurrence_matrix <- co_occurrence_matrix / rowSums(co_occurrence_matrix)
co_occurrence_matrix <- co_occurrence_matrix / colSums(co_occurrence_matrix)
# Create the heatmap
heatmap(co_occurrence_matrix,
col = colorRampPalette(c("blue", "white", "red"))(100),  # Color scheme
main = "Co-occurrence Heatmap",  # Title of the heatmap
xlab = "Tokens",  # Label for x-axis
ylab = "Tokens",   # Label for y-axis
scale = "none"
)
## Histogram
hist(top40terms, main = "Word Frequency Distribution", xlab = "Frequency", ylab = "Count")
## Bar plot
top10terms <- sort(document_topics$terms[topicToViz,], decreasing=TRUE)[1:10]
# Create a data frame with the top 40 terms and their frequencies
top10terms_df <- data.frame(Terms = names(top10terms), Frequency = top10terms, stringsAsFactors = FALSE)
# Round the frequencies to two decimal places
top10terms_df$Frequency <- round(top10terms_df$Frequency, 2)
# Create the bar plot using ggplot2
barchart <- ggplot(top10terms_df, aes(x = reorder(Terms, -Frequency), y = Frequency)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Top 10 Most Frequent Words", x = "Words", y = "Frequency") +
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
geom_text(aes(label = Frequency), vjust = -0.5, color = "black", size = 3)  #
ggsave("barchart.png", plot = barchart)
ggsave("barchart.png", plot = barchart, width = 9, height = 8)
ggsave("barchart.png", plot = barchart, width = 6, height = 5)
## NRC
tidy_data <- no_df %>%
unnest_tokens(word, No_Campaign_Cleaned)
sentiment_scores <- get_nrc_sentiment(tidy_data$word)
# Get total sentiment by sentiment category
total_sentiment <- colSums(sentiment_scores)
# Create dataframe
sentiment_nrc_df <- data.frame(Sentiment_Category = colnames(sentiment_scores), Total_Sentiment = total_sentiment)
# Sort dataframe by total sentiment in descending order
sentiment_nrc_df <- sentiment_nrc_df[order(sentiment_nrc_df$Total_Sentiment, decreasing = TRUE), ]
write.csv(sentiment_nrc_df, "sentiment_nrc_scores.csv")
## Vader
sentiment_vader <- vader_df(tidy_data$word)
# Calculate total frequency for each sentiment category
sentiment_totals <- sentiment_vader %>%
summarise(Total_Compound = sum(compound),
Total_Positive = sum(pos),
Total_Neutral = sum(neu),
Total_Negative = sum(neg))
# Create a dataframe with sentiment categories and total frequencies
sentiment_vader_df <- data.frame(Sentiment_Category = c("Compound", "Positive", "Neutral", "Negative"),
Total_Frequency = c(sentiment_totals$Total_Compound,
sentiment_totals$Total_Positive,
sentiment_totals$Total_Neutral,
sentiment_totals$Total_Negative))
write.csv(sentiment_vader_df, "sentiment_vader_scores.csv")
# Plot results
sentiment_nrc_plot <- ggplot(sentiment_nrc_df, aes(x = Sentiment_Category, y = Total_Sentiment)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Distribution of Sentiment Categories",
x = "Sentiment Category",
y = "Total Sentiment") +
theme_bw()
sentiment_nrc_plot
ggsave("sentiment_nrc_plot.png", plot = sentiment_nrc_plot)
# Plot results
sentiment_vader_plot <- ggplot(sentiment_vader_df, aes(x = Sentiment_Category, y = Total_Frequency)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Distribution of Sentiment Categories",
x = "Sentiment Category",
y = "Total Sentiment") +
theme_bw()
ggsave("sentiment_vader_plot.png", plot = sentiment_vader_plot)
library(easypackages)
libraries("tidyverse", "rvest", "httr", "pdftools", "stringr", "topicmodels", "tidytext")
# Set the path to the folder containing the PDF files
folder_path <- "/Users/elizabethchan/Documents/SICSS/prop22/nexis/Narrow Set 1"
# Get the list of PDF files in the folder
pdf_files <- list.files(folder_path, pattern = "*.pdf", full.names = TRUE)
# Function to extract text from a PDF file
extract_text <- function(file_path) {
text <- pdf_text(file_path)
text <- paste(text, collapse = " ")  # Combine text elements into a single string
text <- str_trim(text)  # Remove leading/trailing whitespaces
return(text)
}
# Create a list to store the results
results <- list()
# Loop through each PDF file, extract text, and add to the list
for (file in pdf_files) {
text <- extract_text(file)
file_name <- basename(file)
results[[file_name]] <- text
}
# Convert the list to a dataframe
df <- data.frame(id = names(results), text = unlist(results), stringsAsFactors = FALSE)
# Replace id column with sequential document assignment
df_new <- df %>% select(-id) %>% mutate(id = row_number()) %>%
select(id, text)
# Rename the rows starting from 1
rownames(df_new) <- 1:nrow(df_new)
# Save new dataframe with all textual data
write.csv(df_new, "textual_data.csv")
textual_data <- read.csv("textual_data.csv") %>% select(-X)
# Create a corpus from the dataframe column
corpus <- Corpus(VectorSource(textual_data$text))
corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE) # Remove punctuation
corpus <- tm_map(corpus, content_transformer(tolower)) # Change to lowercase
corpus <- tm_map(corpus, content_transformer(function(x) gsub("\\s+", " ", x))) # Remove extra spaces
# Remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# Convert back to a dataframe
cleaned_text <- sapply(corpus, as.character)
textual_data$text <- cleaned_text
# Remove numbers using gsub
textual_data$text <- gsub("\\d+", "", textual_data$text)
# Remove specific words: pm
textual_data$text <- gsub("\\bpm\\b", "", textual_data$text)
# Remove specific symbols: "•"
# Remove multiple symbols like • and *
textual_data$text <- gsub("[•*]", "", textual_data$text)
# Create new corpus with cleaned data
corpus_cleaned <- Corpus(VectorSource(textual_data$text))
# Create the Document-Term Matrix
dtm <- DocumentTermMatrix(corpus_cleaned)
# Convert the DTM to a matrix
dtm_matrix <- as.matrix(dtm)
## LDA
# Create the LDA model
lda_model <- LDA(dtm, k = 3, beta = 0.01, control = list(alpha = 0.1)) #  A low alpha value (e.g., 0.1) encourages documents to contain a mixture of fewer dominant topics; A low beta value (e.g., 0.01) makes each topic more focused on a smaller set of highly probable words
# Print the model summary
print(lda_model)
# Get the topics and associated words
topics <- terms(lda_model, 10) # Extract the top 10 terms for each of the 5 topics
topicNames <- apply(topics, 2, paste, collapse=" ")
print(topics)
# Get the top 10 terms for each topic (word-topic probabilities) and plot it
topic_prob <- tidy(lda_model, matrix = "beta")
top_prob_terms <- topic_prob %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
wordtopic_prob_plot <- top_prob_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
theme_bw()
ggsave("wordtopic_prob_plot.png", plot = wordtopic_prob_plot)
# Get the per-document-per-topic probabilities and plot it
document_prob <- tidy(lda_model, matrix = "gamma")
document_prob
## Create a wordcloud
# Visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
png("wordcloud.png")
wordcloud <- wordcloud(top_prob_terms$term, top_prob_terms$beta, random.order = FALSE, color = mycolors)
dev.off()
# Get the top 10 terms overall
top_terms_overall <- topic_prob %>%
slice_max(beta, n = 10) %>%
arrange(topic, -beta)
wordtopic_overall_plot <- top_terms_overall %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(beta, term)) +
geom_col() +
scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
theme_bw()
ggsave("wordtopic_overall_plot.png", plot = wordtopic_overall_plot)
## NRC
tidy_data <- textual_data %>%
unnest_tokens(word, text)
sentiment_scores <- get_nrc_sentiment(tidy_data$word)
saveRDS(tidy_data, "tidy_data.RDS")
## Vader
sentiment_vader <- vader_df(tidy_data$word)
library(easypackages)
libraries("tidyverse", "rvest", "httr", "tm", "topicmodels", "RColorBrewer", "wordcloud", "tidytext", "syuzhet", "gplots", "ggplot2")
no_df <- readRDS("no_df.csv")
no_df <- readRDS("no_df.csv")
corpus <- Corpus(VectorSource(no_df$No_Campaign))
# Preprocessing: remove punctuation and convert to lowercase
corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE) # Remove punctuation
corpus <- tm_map(corpus, content_transformer(tolower)) # Change to lowercase
corpus <- tm_map(corpus, content_transformer(function(x) gsub("\\s+", " ", x))) # Remove extra spaces
# Remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# Convert back to a dataframe
cleaned_no_text <- sapply(corpus, as.character)
no_df$No_Campaign_Cleaned <- cleaned_no_text
library(easypackages)
libraries("tidyverse", "rvest", "httr", "pdftools", "stringr", "topicmodels", "tidytext")
tidy_data <- readRDS("tidy_data.RDS")
View(tidy_data)
tidy_data_new <- tidy_data %>% distinct()
View(tidy_data_new)
sentiment_scores <- get_nrc_sentiment(tidy_data_new$word)
# Load packages
library(easypackages)
libraries("dplyr", "data.table", "tm", "tidyr", "tidytext", "textdata", "vader", "quanteda", "readtext", "ggplot2", "text2vec", "quanteda.textstats", "quanteda.textplots", "remotes")
install_github("EmilHvitfeldt/textdata")
install_github("juliasilge/tidytext")
get_sentiments("nrc")
textdata::lexicon_nrc(delete = TRUE)
textdata::lexicon()
########################################
## nrc
########################################
nrc_words <- cleaned_tweet_words %>%
inner_join(get_sentiments("nrc"), # the nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
by = c("word")) # 229,127
dir <- "~/Documents/SICSS/sentiment analysis/"
setwd(dir)
# read timeline data
tl <- fread(paste0(dir,"tl1_anon.csv"))
View(tl)
# read user data
user <- fread(paste0(dir, "user.csv"))
View(user)
names(tl)[names(tl) == 'V1'] <- 'tweetid'
names(tl)
dim(tl)
# randomly select 500 users for example
# sentiment analysis can take a long time...
# (running vader on the full data set takes 1.6 days)
set.seed(8675309)
tl_small <- tl[tl$screenName.anon %in% sample(unique(screenName.anon), 500), ]
dim(tl_small)
head(tl_small)
# clean up // remove URLS and "&"
tl_small$tweets <- gsub("http.*","", tl_small$text)
tl_small$tweets <- gsub("https.*","", tl_small$tweets)
tl_small$tweets <- gsub("&amp","", tl_small$tweets)
# "tokenize" tweets (breaks tweets down into words)
tl_tweets_clean <- tl_small %>%
dplyr::select(tweets, tweetid) %>%
unnest_tokens(word, tweets)
head(tl_tweets_clean)
data("stop_words")
custom_stop_words <- bind_rows(stop_words,
tibble(word = tm::stopwords("spanish"),
lexicon = "custom"))
nrow(tl_tweets_clean) ## number of words = 1,241,184
cleaned_tweet_words <- tl_tweets_clean %>%
anti_join(custom_stop_words)
head(cleaned_tweet_words)
nrow(cleaned_tweet_words) ## number of words = 651,941
########################################
## nrc
########################################
nrc_words <- cleaned_tweet_words %>%
inner_join(get_sentiments("nrc"), # the nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
by = c("word")) # 229,127
View(nrc_words)
sentiment_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), # the nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
by = c("word"))
## NRC
tidy_data <- textual_data %>%
unnest_tokens(word, text)
tidy_data_new <- readRDS("tidy_data.RDS")
sentiment_scores <- tidy_data_new %>%
inner_join(get_sentiments("nrc"), # the nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
by = c("word"))
View(sentiment_scores)
View(nrc_words)
## NRC
tidy_data <- textual_data %>%
unnest_tokens(word, text)
textual_data <- read.csv("textual_data.csv") %>% select(-X)
## NRC
tidy_data <- textual_data %>%
unnest_tokens(word, text)
sentiment_nrc_scores <- get_nrc_sentiment(tidy_data$word)
nrc_vector <- get_sentiment(tidy_data, method = "nrc", lang = "english")
nrc_vector
sum(nrc_vector)
mean(nrc_vector)
summary(nrc_vector)
## NRC
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), # the nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
by = c("word"))
View(sentiment_nrc_scores)
# nrc scores each word as post/negative as well as other categories
sentiment_nrc_scores$sentiment.nrc.score <- 0
sentiment_nrc_scores$sentiment.nrc.score[sentiment_nrc_scores$sentiment == "positive"] <- 1
sentiment_nrc_scores$sentiment.nrc.score[sentiment_nrc_scores$sentiment == "negative"] <- -1
# sum sentiment scores for each tweet
sentiment_nrc_scores <- sentiment_nrc_scores %>%
group_by(id) %>%
summarize(sentiment.nrc.score=sum(sentiment.nrc.score)) %>%
data.frame
View(sentiment_nrc_scores)
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), # the nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
by = c("word"))
# nrc scores each word as post/negative as well as other categories
sentiment_nrc_scores$sentiment.nrc.score <- 0
sentiment_nrc_scores$sentiment.nrc.score[sentiment_nrc_scores$sentiment == "positive"] <- 1
sentiment_nrc_scores$sentiment.nrc.score[sentiment_nrc_scores$sentiment == "negative"] <- -1
# Sum sentiment scores for each document
sentiment_nrc_scores <- sentiment_nrc_scores %>%
group_by(id) %>%
mutate(sentiment.nrc.score=sum(sentiment.nrc.score)) %>%
data.frame
# Sort dataframe by total sentiment in descending order
sentiment_nrc_scores <- sentiment_nrc_scores[order(sentiment_nrc_scores$sentiment.nrc.score, decreasing = TRUE), ]
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), # the nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
by = c("word"))
# nrc scores each word as post/negative as well as other categories
sentiment_nrc_scores$sentiment.nrc.score <- 0
sentiment_nrc_scores$sentiment.nrc.score[sentiment_nrc_scores$sentiment == "positive"] <- 1
sentiment_nrc_scores$sentiment.nrc.score[sentiment_nrc_scores$sentiment == "negative"] <- -1
# Sum sentiment scores for each document
sentiment_nrc_scores <- sentiment_nrc_scores %>%
group_by(id) %>%
mutate(sentiment.nrc.score.total=sum(sentiment.nrc.score)) %>%
data.frame
# Sort dataframe by total sentiment in descending order
sentiment_nrc_scores <- sentiment_nrc_scores[order(sentiment_nrc_scores$sentiment.nrc.score.total, decreasing = TRUE), ]
test <- sentiment_nrc_scores %>% select(sentiment) %>% distinct()
View(test)
## NRC
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), # the nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
by = c("word"))
View(sentiment_nrc_scores)
test <- get_sentiments("nrc")
View(test)
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc")) %>%
count(id, index = linenumber %/% 80, sentiment)
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc")) %>%
count(index = linenumber %/% 80, sentiment)
s
## NRC
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), # the nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
by = c("word"))
View(sentiment_nrc_scores)
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), by = c("word")) %>%
group_by(id) %>%
summarize(sentiment = sum(sentiment.nrc.score[which(sentiment == "positive")]) - sum(sentiment.nrc.score[which(sentiment == "negative")]))
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), by = c("word"))
View(sentiment_nrc_scores)
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), by = c("word"))
sentiment_nrc_scores$sentiment.nrc.score <- 0
sentiment_nrc_scores$sentiment.nrc.score[sentiment_nrc_scores$sentiment == "positive"] <- 1
sentiment_nrc_scores$sentime
sentiment_nrc_scores <- sentiment_nrc_scores %>%
group_by(id) %>%
summarize(sentiment = sum(sentiment.nrc.score[which(sentiment == "positive")]) - sum(sentiment.nrc.score[which(sentiment == "negative")]))
sentiment_nrc_scores <- tidy_data %>%
inner_join(get_sentiments("nrc"), by = c("word"))
# nrc scores each word as post/negative as well as other categories
sentiment_nrc_scores$sentiment.nrc.score <- 0
sentiment_nrc_scores$sentiment.nrc.score[sentiment_nrc_scores$sentiment == "positive"] <- 1
sentiment_nrc_scores$sentiment.nrc.score[sentiment_nrc_scores$sentiment == "negative"] <- -1
sentiment_nrc_scores <- sentiment_nrc_scores %>%
group_by(id) %>%
mutate(sentiment = sum(sentiment.nrc.score[which(sentiment == "positive")]) - sum(sentiment.nrc.score[which(sentiment == "negative")]))
# Sum sentiment scores for each document
sentiment_nrc_scores_1 <- sentiment_nrc_scores %>%
group_by(id) %>%
mutate(sentiment.nrc.score.total=sum(sentiment.nrc.score)) %>%
data.frame
View(sentiment_nrc_scores_1)
119-39
79-37
library(easypackages)
libraries("tidyverse", "rvest", "httr", "pdftools", "stringr", "topicmodels", "tidytext")
# Set the path to the folder containing the PDF files
folder_path <- "/Users/elizabethchan/Documents/SICSS/prop22/nexis/Narrow Set 1"
# Get the list of PDF files in the folder
pdf_files <- list.files(folder_path, pattern = "*.pdf", full.names = TRUE)
# Function to extract text from a PDF file
extract_text <- function(file_path) {
text <- pdf_text(file_path)
text <- paste(text, collapse = " ")  # Combine text elements into a single string
text <- str_trim(text)  # Remove leading/trailing whitespaces
return(text)
}
# Create a list to store the results
results <- list()
# Loop through each PDF file, extract text, and add to the list
for (file in pdf_files) {
text <- extract_text(file)
file_name <- basename(file)
results[[file_name]] <- text
}
# Convert the list to a dataframe
df <- data.frame(id = names(results), text = unlist(results), stringsAsFactors = FALSE)
# Replace id column with sequential document assignment
df_new <- df %>% select(-id) %>% mutate(id = row_number()) %>%
select(id, text)
